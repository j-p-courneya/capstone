---
title: "Coursera Data Science - Milestone Report"
author: "Jean-Paul Courneya"
date: "3/26/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#Background

The motivation for this report is to: 

1. Demonstrate that data has been downloaded and successfully loaded.

2. Create a basic report of summary statistics about the data sets.

3. Report any interesting findings on the data so far.

```{r, eval=FALSE}
if(!file.exists("data")) {
  dir.create("data")
}

dataURL <- "https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip"
download.file(dataURL, destfile = "./data")
```

```{r}
if(!file.exists("./final/en_US/en_US.news.txt")||
     !file.exists("./final/en_US/en_US.blogs.txt")||    
     !file.exists("./final/en_US/en_US.twitter.txt"))
  {  
    if(!file.exists("Coursera-SwiftKey.zip"))
      {    
        download.file(url="https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip",  destfile="Coursera-SwiftKey.zip", quiet=T)
      }
    unzip(zipfile="Coursera-SwiftKey.zip")
  }
```


#Data Import and Summary Statistics

The data for the project comes from corpora that were collected from publicly available sources by a web crawler. For this analysis the three files in English will be used. The data was downloaded to the working directory and analyzed from there.

```{r}
list.files(pattern = "^en_US.*txt$",path = "./final/en_US")
```

Using `readLines` the text files are read as saved as character vectors.

```{r}
blogs <- readLines("./final/en_US/en_US.blogs.txt", encoding = "UTF-8", skipNul = TRUE)
news <- readLines("./final/en_US/en_US.news.txt", encoding = "UTF-8", skipNul = TRUE)
twitter <- readLines("./final/en_US/en_US.twitter.txt", encoding = "UTF-8", skipNul = TRUE)
```

What do these look like? Next we will take a look at the `head` of each of the three files. 

```{r}
head(blogs, n = 3)
```

```{r}
head(news, n = 3)
```

```{r}
head(twitter, n = 3)
```

Here the goal is to find out the size of the files, # of lines, and words in each.

```{r}
## Size of each file
size <- round(file.info(c("./en_US/en_US.blogs.txt", 
                          "./en_US/en_US.news.txt", 
                          "./en_US/en_US.twitter.txt"))$size/1024/1024, 2)
## Number of lines in each file
lines <- c(length(blogs), 
           length(news), 
           length(twitter))
## Number of characters in each file
char <- c(sum(nchar(blogs)), 
          sum(nchar(news)), 
          sum(nchar(twitter)))
## Number of words
library(stringr)
words <- c(sum(str_count(blogs, "\\S+")), 
           sum(str_count(news, "\\S+")), 
           sum(str_count(twitter, "\\S+")))
## Knit results into a data table
library(knitr)
stats <- cbind(size, lines, char, words)
colnames(stats) <- c("File Size (MB)", "Lines", "Characters", "Words")
rownames(stats) <- c("Blogs", "News", "Twitter")
kable(stats)
```

In the next section we visualize the line counts and word counts in the documents

```{r}
# plot prep
summaryStats <- as.data.frame(stats)
library(ggplot2)
g.line.count <- ggplot(summaryStats, aes(x = factor(rownames(stats)), y = lines/1e+06))
g.line.count <- g.line.count + geom_bar(stat = "identity") +
  labs(y = "# of lines/million", x = "text source", title = "Count of lines per Corpus") 
# g.line.count
g.word.count <- ggplot(summaryStats, aes(x = factor(rownames(stats)), y = words/1e+06))
g.word.count <- g.word.count + geom_bar(stat = "identity") + 
  labs(y = "# of words/million", x = "text source", title = "Count of words per Corpus")
```

# Preparing the Corpus 

To accomodate for insufficient processing performance in my local computing environment a sample of each file is made before proceeding to tokenize and create n-grams.
```{r}
#Create a sample corpus to be processed
blogs_sample <- blogs[sample(length(blogs), 0.1*length(blogs))]
news_sample <- news[sample(length(news), 0.1*length(news))]
twitter_sample <- twitter[sample(length(twitter), 0.1*length(twitter))]
combinedSampleCorpus <- quanteda::corpus(c(blogs_sample, news_sample, twitter_sample))
```


Now a document-feature matrix (aka document-term matrix), is created describing the frequency of n-grams, or sequences of n words. We use the dfm function, which tokenizes the stream of text into n-grams; converts the text to lowercase; and removes numbers, punctuation, and whitespaces.

At this stage, it is not uncommon to remove stopwords - since the purpose of this project is to create a predictive text model, they will be kept. However, we filter for profane language, using a list of words available at [LDNOOBW](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words/blob/master/LICENSE) the profane words will be removed during building the `quanteda dfm`

```{r}
library(quanteda)
profanity <- readLines("profaneWords.txt", encoding = "UTF-8", skipNul = TRUE)
```

Create a `quanteda dfm` using the `corpus_sample`
```{r}
corpusDFM <- dfm(corpus_sample,remove = profanity, stem = FALSE, remove_punct = TRUE,  ngrams=c(1,2,3))
```

```{r}
#blogs_corpus <- corpus(blogs_sample)
#news_corpus <- corpus(news_sample)
#twitter_corpus <- corpus(twitter_sample)
```

```{r}
corpusTokens <- tokens(cSC, remove_numbers = TRUE, remove_punct = TRUE, remove_separators = TRUE)
corpusUnigrams <- tokens_ngrams(corpusTokens, n= 1L)
corpusBigrams <- tokens_ngrams(corpusTokens, n=2L)
corpusTrigrams <- tokens_ngrams(corpusTokens, n=3L)
dfmUni <- dfm(corpusUnigrams,remove = profanity, stem = FALSE)
dfmBi <- dfm(corpusBigrams,remove = profanity, stem = FALSE)
dfmTri <- dfm(corpusTrigrams, remove = profanity, stem = FALSE)
```

```{r}
corpusStats<- textstat_frequency(corpusDFM)
```


Examine features of the corpus prior to sampling. create a document feature matrix and graph the top 20 uni, bi, tri, and quartenary n-grams. Also while creating the `dfm` profanity will be removed.

```{r}
library(ggplot2)
library(ggthemes)
for (i in 1:4) {
        ## Prepare data frame for plotting
        graphData <- as.data.frame(
                topfeatures(dfm(corpus_sample,remove = profanity, stem = FALSE, remove_punct = TRUE,  ngrams=i)
, 20)
                )
        colnames(graphData) <- "frequency"
        graphData$ngram <- row.names(graphData)
        ## Generate plots 
        g <- ggplot(graphData, aes(y = frequency, 
                                   x = reorder(ngram, frequency)))
        g <- g + geom_bar(stat = "identity") + coord_flip()
        g <- g + ggtitle(paste(i, "-grams", sep = "")) 
        g <- g + ylab("") + xlab("")
        g <- g + theme_few()
        assign(paste("p", i, sep = ""), g)
}
## Combine plots
multiplot(p1, p2, p3, p4, cols=2)
```



```{r}
data <- as.data.frame(topfeatures(corpusDFM, n = length(featnames(corpusDFM))))
colnames(data) <- "freq"
data$ngram <- row.names(data)
row.names(data) <- c(1:nrow(data))
data$rank <- c(1:nrow(data))  ## Word frequency by rank
data$pct_total <- data$freq / sum(data$freq) * 100 ## Word frequency as percent of total
data$pct_cumul <- cumsum(data$pct_total) ## Word frequency as cumulative % of total
knitr::kable(data[1:20, c(2, 3, 1, 4, 5)],
      col.names = c("N-gram", "Rank", "Frequency", "% of Total", "Cumulative %"))

```

Load the multiplot function found at [multiplot function source](http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/)
```{r}
# Multiple plot function
#
# ggplot objects can be passed in ..., or to plotlist (as a list of ggplot objects)
# - cols:   Number of columns in layout
# - layout: A matrix specifying the layout. If present, 'cols' is ignored.
#
# If the layout is something like matrix(c(1,2,3,3), nrow=2, byrow=TRUE),
# then plot 1 will go in the upper left, 2 will go in the upper right, and
# 3 will go all the way across the bottom.
#
multiplot <- function(..., plotlist=NULL, file, cols=1, layout=NULL) {
library(grid)
# Make a list from the ... arguments and plotlist
plots <- c(list(...), plotlist)
numPlots = length(plots)
# If layout is NULL, then use 'cols' to determine layout
if (is.null(layout)) {
# Make the panel
# ncol: Number of columns of plots
# nrow: Number of rows needed, calculated from # of cols
layout <- matrix(seq(1, cols * ceiling(numPlots/cols)),
ncol = cols, nrow = ceiling(numPlots/cols))
}
if (numPlots==1) {
print(plots[[1]])
} else {
# Set up the page
grid.newpage()
pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))
# Make each plot, in the correct location
for (i in 1:numPlots) {
# Get the i,j matrix positions of the regions that contain this subplot
matchidx <- as.data.frame(which(layout == i, arr.ind = TRUE))
print(plots[[i]], vp = viewport(layout.pos.row = matchidx$row,
layout.pos.col = matchidx$col))
}
}
}
```


